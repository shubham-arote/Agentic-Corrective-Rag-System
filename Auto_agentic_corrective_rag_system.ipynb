{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1xUNfa6LaS5AsuAGbWC1JZsLj6s-78bIe",
      "authorship_tag": "ABX9TyPiQXGiGy24R3zgukALY48W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-arote/Agentic-Corrective-Rag-System/blob/main/Auto_agentic_corrective_rag_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building agentic corrective RAG System with LangGraph\n"
      ],
      "metadata": {
        "id": "2NyzFNQO6Iui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install langchain-community\n",
        "!pip install langchain-google-genai\n",
        "!pip install langchain-chroma"
      ],
      "metadata": {
        "id": "pAICJH7F7KIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['WEATHER_API_KEY'] = userdata.get('WEATHER_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GEMINI_API_KEY')\n",
        "os.environ['TAVILY_API_KEY']=userdata.get('TAVILY_API_KEY')\n",
        "os.environ['HUGGINGFACE_API_KEY']=userdata.get('HUGGINGFACE_API_KEY')"
      ],
      "metadata": {
        "id": "k5k12UxtZ_rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --quiet  langchain sentence_transformers"
      ],
      "metadata": {
        "id": "9mEZ4tyfl-PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "Wr7c7sexlreI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build search Index for wikipedia data\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "VdD6DWfabqgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "id": "871cx6uAkNTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "chatgpt = ChatGroq(\n",
        "    model = \"llama3-8b-8192\",\n",
        "    temperature = 0\n",
        ")"
      ],
      "metadata": {
        "id": "z6x4u3dYkMOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPu3ETx4IQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "wikipidea_filepath = '/content/drive/MyDrive/simplewiki-2020-11-01.jsonl.gz'\n",
        "docs = []\n",
        "with gzip.open(wikipidea_filepath, 'rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line.strip())\n",
        "        #Add all paragraphs\n",
        "        docs.append(\n",
        "            {\n",
        "                'metadata': {\n",
        "                    'title': data.get('title'),\n",
        "                    'article_id': data.get('id')\n",
        "\n",
        "                },\n",
        "                'data': ' '.join(data.get('paragraphs')[0:3]) # restrict data to first three paragraphs\n",
        "            }\n",
        "        )\n",
        "\n",
        "#we subset ourt data so only use a subset of wikipedia documents\n",
        "docs = [doc for doc in docs for x in ['india']\n",
        "        if x in doc['data'].lower().split()]\n",
        "\n",
        "docs = [Document(page_content=doc['data'], metadata=doc['metadata']) for doc in docs]\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "print(\"chunks:\", len(chunked_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e708XucAeGmB",
        "outputId": "5c52932a-c5e2-4dee-95c1-73c8fb08336a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunks: 1322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_docs[0:3]"
      ],
      "metadata": {
        "id": "9rfZqLeYmuiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                  collection_name= 'rag_wkipedia_db',\n",
        "                                  embedding=embedding_model,\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./wikipedia_db\")"
      ],
      "metadata": {
        "id": "XaFmw63Beyes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                                       search_kwargs={\"k\": 3,\n",
        "                                                                    \"score_threshold\": 0.3})"
      ],
      "metadata": {
        "id": "E9VjM0CagXDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capital of India?\"\n",
        "top3docs = similarity_threshold_retriever.get_relevant_documents(query)\n",
        "top3docs"
      ],
      "metadata": {
        "id": "gqfn24BWhLWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Query Retrieval Grader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel ,Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#Data model for LLM output format\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved document\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# prompt template for grading\n",
        "SYS_PROMPT = \"\"\"You are an expert grader assessing relevance of a retrieved document to a user question\n",
        "                Follow these instructions for grading:\n",
        "                -If the document contains keywords(s) or semantic meaning related to question ,grade it as relevant.\n",
        "                -Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not\n",
        "            \"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Retrieved document:\n",
        "                    {document}\n",
        "                    User Question:\n",
        "                    {question}\n",
        "                    \"\"\"),\n",
        "    ]\n",
        ")\n",
        "#Build grader chain\n",
        "\n",
        "doc_grader =(grade_prompt | structured_llm_grader)"
      ],
      "metadata": {
        "id": "J34GGI9Ohi-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capital of pakistan?\"\n",
        "top3docs = similarity_threshold_retriever.get_relevant_documents(query)\n",
        "for doc in top3docs:\n",
        "    print(doc.page_content)\n",
        "    print('GRADE:',doc_grader.invoke({\"question\": query,\"document\": doc.page_content,}))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Fl4FyAA2lWUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who won the champions league in 2024\"\n",
        "top3docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in top3docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc_grader.invoke({\"document\": doc.page_content, \"question\": query}))\n",
        "    print()"
      ],
      "metadata": {
        "id": "qt0biRghmRy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build QA rag chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering task\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know the answer.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detaield answer and to the point answer with regard to the question\n",
        "            Question:\n",
        "            {question}\n",
        "            Context:\n",
        "            {context}\n",
        "            Answer:\n",
        "        \"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "chat_gpt = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "qa_rag_chain =(\n",
        "    {\n",
        "        \"context\": (itemgetter('context')\n",
        "                        |\n",
        "                    RunnableLambda(format_docs)),\n",
        "        \"question\": itemgetter('question')\n",
        "    }\n",
        "     |\n",
        "    prompt_template\n",
        "     |\n",
        "    chatgpt\n",
        "     |\n",
        "    StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "9_SzkWjkmqnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is capital of pakistan\"\n",
        "top3docs = similarity_threshold_retriever.invoke(query)\n",
        "result =qa_rag_chain.invoke({\"context\": top3docs,\"question\": query})\n",
        "print(result)\n",
        "type(result)"
      ],
      "metadata": {
        "id": "36qEwREC_4kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Query Rephraser\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0)\n",
        "#prompt template\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a question re-writer and perform the following task:\n",
        "                -convert the following input question to a better version that is optimized for web search\n",
        "                -when re-writing the question, look at the input question and try to reason about the underlying semantic meaning\n",
        "                -generate only one question in string format\n",
        "            \"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Here is initial question:\n",
        "                    {question}\n",
        "                    formulate an improved question\n",
        "                \"\"\")\n",
        "    ]\n",
        ")\n",
        "question_rewriter = (re_write_prompt | llm | StrOutputParser())"
      ],
      "metadata": {
        "id": "gYG4ty01rUtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who won the champions league in 2024\"\n",
        "question_rewriter.invoke({\"question\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ClArGhotBeYI",
        "outputId": "c52f9214-ce80-4f9d-c853-720eb1271475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Who is the predicted winner of the 2024 Champions League?\" \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --quiet  duckduckgo-search"
      ],
      "metadata": {
        "id": "62SMqEMQULiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tv_search = TavilySearchResults(max_results=3, search_depth= 'advanced',\n",
        "                                    max_tokens =10000)"
      ],
      "metadata": {
        "id": "Ntc-HWVHUi-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search"
      ],
      "metadata": {
        "id": "WAFwIVMNI_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load web search tool\n",
        "\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "search = DuckDuckGoSearchResults(max_results=3, search_depth='advanced',\n",
        "                                max_tokens=1000)"
      ],
      "metadata": {
        "id": "hwLXpqL0uIv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs =search.invoke(\"who won the champions league in 2024\")\n",
        "docs"
      ],
      "metadata": {
        "id": "opzCz4u6JZZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXoAb9AJNHF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph State"
      ],
      "metadata": {
        "id": "9Q_5mGlmBxcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph\n",
        "\n",
        "    Attributes:\n",
        "        qustion: quetions\n",
        "        generation: LLM response generation\n",
        "        web_search_needed: whether to add web search\n",
        "        documents: list of context document\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search_needed: str\n",
        "    documents: List[str]\n"
      ],
      "metadata": {
        "id": "ePnuIJ8jvsQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retreive funtion  for retrival from vector df\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "    Args:\n",
        "        state(dict): The current graph state\n",
        "    Returns:\n",
        "        state(dict): New key added to state, documents - that contains the retrieved context documents\n",
        "    \"\"\"\n",
        "    print(\"--RETRIEVAL FROM VECTOR DETABASE --\")\n",
        "    question =state[\"question\"]\n",
        "    documents = similarity_threshold_retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n"
      ],
      "metadata": {
        "id": "vPDWweCpyaWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grade documents\n",
        "def grade_documents(state):\n",
        "        \"\"\"\n",
        "        Determines whether the retrieved documents are relevant to the question\n",
        "        by using an LLM Grader\n",
        "        If any document are not relevant to the question or documents are empty - Web search needs to be done\n",
        "        If all documents are relevant to the question web search is not needed\n",
        "        Helps filtering out irrelevant documents\n",
        "        Args:\n",
        "            state(dict): The current graph state\n",
        "        Returns:\n",
        "            state(dict): Updates document key with only filtered documents\n",
        "        \"\"\"\n",
        "        print(\"--CHECK DOCUMENT RELEVANCE TO QUESTION\")\n",
        "        question = state[\"question\"]\n",
        "        documents = state[\"documents\"]\n",
        "\n",
        "        #score each document\n",
        "        filtered_docs =[]\n",
        "        web_search_needed = \"No\"\n",
        "        if documents:\n",
        "            for d in documents:\n",
        "                score = doc_grader.invoke(\n",
        "                    {\"question\": question, \"document\": d.page_content})\n",
        "                grade = score.binary_score\n",
        "                if grade == \"yes\":\n",
        "                    print(\"--GRADE: DOCUMENT RELEVANT--\")\n",
        "                    filtered_docs.append(d)\n",
        "                else:\n",
        "                    print(\"--GRADE: DOCUMENT NOT RELEVANT\")\n",
        "                    web_search_needed = \"Yes\"\n",
        "                    continue\n",
        "        else:\n",
        "            print(\"--NO DOCUMENTS RETRIEVED--\")\n",
        "            web_search_needed = \"Yes\"\n",
        "        return {\"documents\":filtered_docs, \"question\":question, \"web_search_needed\": web_search_needed}"
      ],
      "metadata": {
        "id": "PSmYR1320cTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rewrite query\n",
        "def rewrite_query(state):\n",
        "    \"\"\"\n",
        "    Rewrite the query to produce better question.\n",
        "\n",
        "    Args:\n",
        "        state(dict): the current graph state\n",
        "    Returns:\n",
        "        state(dict): Updates question key with a rephrased or re-written question\n",
        "    \"\"\"\n",
        "    print(\"--REWRITE QUERY--\")\n",
        "    question = state[\"question\"]\n",
        "    documents= state[\"documents\"]\n",
        "\n",
        "    # rewrite question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "    print(better_question)\n"
      ],
      "metadata": {
        "id": "7BvkgabA4GeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Web Search\n",
        "from langchain.schema import Document\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-written question.\n",
        "\n",
        "    Args:\n",
        "        state(dict): The current graph state\n",
        "    Returns:\n",
        "        state(dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "    print(\"--WEB SEARCH--\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    #web search\n",
        "    docs = tv_search.invoke(question)\n",
        "    web_result = \"\\n\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_result = Document(page_content= web_result)\n",
        "    documents.append(web_result)\n",
        "    return {\"documents\": documents, \"question\": question}\n"
      ],
      "metadata": {
        "id": "vT_mNxYi5HLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tlY7earBEiBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Answer\n",
        "def generate_answer(state):\n",
        "    \"\"\"\n",
        "    Generate answer from context document using LLM\n",
        "\n",
        "    Args:\n",
        "        state(dict): The current graph state\n",
        "    Returns:\n",
        "        state(dict): New key added to state, generation , that contains the LLM response\n",
        "    \"\"\"\n",
        "    print(\"--GENERATE ANSWER--\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question })\n",
        "    return {\"documents\":documents,\"question\": question,\"generation\": generation }\n"
      ],
      "metadata": {
        "id": "gMWwt9JOHjKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oIrNx_X_hEA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decide to  generate\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question\n",
        "\n",
        "    Arg:\n",
        "        state(dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "    print(\"---ASSESS GRADED DOCUMENTS--\")\n",
        "    web_search_needed = state[\"web_search_needed\"]\n",
        "\n",
        "    if web_search_needed == \"Yes\":\n",
        "    #All documents have been filtered check_relevance\n",
        "    #We will re-generate new query\n",
        "        print(\"---DECISION: SOME OR ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY--\")\n",
        "        return \"rewrite_query\"\n",
        "    else:\n",
        "    #we have relevant documents so generate answer\n",
        "        print(\"--DECISION: GENERATE RESPONSE--\")\n",
        "        return \"generate_answer\"\n"
      ],
      "metadata": {
        "id": "aMOcEye6d3gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the agent graph\n",
        "from langgraph.graph import END, StateGraph\n",
        "agentic_rag = StateGraph(GraphState)\n",
        "\n",
        "#Define the nodes\n",
        "agentic_rag.add_node(\"retrieve\", retrieve)\n",
        "agentic_rag.add_node(\"grade_documents\", grade_documents)\n",
        "agentic_rag.add_node(\"rewrite_query\",  rewrite_query)\n",
        "agentic_rag.add_node(\"web_search\", web_search)\n",
        "agentic_rag.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "#Build graph\n",
        "agentic_rag.set_entry_point(\"retrieve\")\n",
        "agentic_rag.add_edge(\"retrieve\", \"grade_documents\")\n",
        "agentic_rag.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\"rewrite_query\": \"rewrite_query\", \"generate_answer\":\"generate_answer\"}\n",
        ")\n",
        "agentic_rag.add_edge(\"rewrite_query\", \"web_search\")\n",
        "agentic_rag.add_edge(\"web_search\", \"generate_answer\")\n",
        "agentic_rag.add_edge(\"generate_answer\", END)\n",
        "\n",
        "agentic_rag = agentic_rag.compile()\n"
      ],
      "metadata": {
        "id": "3THXUZb4IxK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image(agentic_rag.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "EVe_eTVvLWGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is langgraph\"\n",
        "response = agentic_rag.invoke({\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-gnGRB1L0cd",
        "outputId": "76ef2ee5-0acc-49c7-c070-532ff17756e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--RETRIEVAL FROM VECTOR DETABASE --\n",
            "--CHECK DOCUMENT RELEVANCE TO QUESTION\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "---ASSESS GRADED DOCUMENTS--\n",
            "---DECISION: SOME OR ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY--\n",
            "--REWRITE QUERY--\n",
            "--WEB SEARCH--\n",
            "--GENERATE ANSWER--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "l64qSPqNMKYM",
        "outputId": "e9b03384-b3ae-4ab7-840c-eebf55a9a765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, LangChain is a unified benchmark that helps AI researchers build models that can leverage real-world knowledge to accomplish a broad range of tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who won icc the world cup in 2024?\"\n",
        "response = agentic_rag.invoke({\"question\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8sOe3gmMZk9",
        "outputId": "76148ea2-8398-4b6b-da8d-a8d2195c9730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--RETRIEVAL FROM VECTOR DETABASE --\n",
            "--CHECK DOCUMENT RELEVANCE TO QUESTION\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "---ASSESS GRADED DOCUMENTS--\n",
            "---DECISION: SOME OR ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY--\n",
            "--REWRITE QUERY--\n",
            "--WEB SEARCH--\n",
            "--GENERATE ANSWER--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "2Ft7b0XGScPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "id": "5LFkTCHdNGoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who won world cup in 2011\"\n",
        "response = agentic_rag.invoke({\"question\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arxxtzJONH1U",
        "outputId": "e7f636ea-dd34-465a-8d69-aaa33900af11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--RETRIEVAL FROM VECTOR DETABASE --\n",
            "--CHECK DOCUMENT RELEVANCE TO QUESTION\n",
            "--GRADE: DOCUMENT RELEVANT--\n",
            "--GRADE: DOCUMENT RELEVANT--\n",
            "--GRADE: DOCUMENT NOT RELEVANT\n",
            "---ASSESS GRADED DOCUMENTS--\n",
            "---DECISION: SOME OR ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY--\n",
            "--REWRITE QUERY--\n",
            "--WEB SEARCH--\n",
            "--GENERATE ANSWER--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "fja35rmWI1ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "oWbsS0DPIAPL",
        "outputId": "51b6c6fd-b1c6-47f0-a396-fdd60bf970d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the provided context, the answer to the question \"Which country won the 2011 Cricket World Cup?\" is:\n\nA India\n\nThe context states: \"India won the tournament defeating Sri Lanka by 6 wickets in the final. India was the first nation to win the Cricket World Cup final on home soil.\""
          },
          "metadata": {}
        }
      ]
    }
  ]
}